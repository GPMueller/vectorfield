#ifdef USE_CUDA

#include <kernel_vectormath.hpp>
#include <Eigen/Dense>

#include <iostream>
#include <stdio.h>

// CUDA Version
namespace Kernel
{
    __global__ void cu_fill(Vector3 *vf1, Vector3 v2, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            vf1[idx] = v2;
        }
    }
    void fill(vectorfield & vf, const Vector3 & v)
    {
        int n = vf.size();
        cu_fill<<<(n+1023)/1024, 1024>>>(vf.data(), v, n);
        cudaDeviceSynchronize();
    }

    __global__ void cu_scale(Vector3 *vf1, scalar sc, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            vf1[idx] *= sc;
        }
    }
    void scale(vectorfield & vf, const scalar & sc)
    {
        int n = vf.size();
        cu_scale<<<(n+1023)/1024, 1024>>>(vf.data(), sc, n);
        cudaDeviceSynchronize();
    }


    __global__ void cu_dot(const Vector3 *vf1, const Vector3 *v2, double *out, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            out[idx] = vf1[idx].dot(v2[idx]);
        }
    }

	scalar dot(const vectorfield & vf1, const vectorfield & vf2)
    {
        int n = vf1.size();
        scalarfield sf(n);
        scalar ret;

        // Dot product
        cu_dot<<<(n+1023)/1024, 1024>>>(vf1.data(), vf2.data(), sf.data(), n);
        cudaDeviceSynchronize();

        // reduction
        for (int i=0; i<n; ++i)
        {
            ret += sf[i];
        }
        return ret;
    }


    // The wrapper for the calling of the actual kernel
    void dot(const vectorfield & vf1, const vectorfield & vf2, scalarfield & s)
    {
        int n = vf1.size();

        // Dot product
        cu_dot<<<(n+1023)/1024, 1024>>>(vf1.data(), vf2.data(), s.data(), n);
        cudaDeviceSynchronize();
    }

    __global__ void cu_cross(const Vector3 *vf1, const Vector3 *vf2, Vector3 *out, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            out[idx] = vf1[idx].cross(vf2[idx]);
        }
    }
    // The wrapper for the calling of the actual kernel
    void cross(const vectorfield & vf1, const vectorfield & vf2, vectorfield & s)
    {
        int n = vf1.size();

        // Dot product
        cu_cross<<<(n+1023)/1024, 1024>>>(vf1.data(), vf2.data(), s.data(), n);
        cudaDeviceSynchronize();
    }




	__global__ void cu_add_c_a(scalar c, Vector3 a, Vector3 * out, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            out[idx] += c*a;
        }
    }
    // out[i] += c*a
	void add_c_a(const scalar & c, const Vector3 & a, vectorfield & out)
    {
        int n = out.size();

        // Dot product
        cu_add_c_a<<<(n+1023)/1024, 1024>>>(c, a, out.data(), n);
        cudaDeviceSynchronize();
    }

    __global__ void cu_add_c_a2(scalar c, const Vector3 * a, Vector3 * out, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            out[idx] += c*a[idx];
        }
    }
    // out[i] += c*a[i]
	void add_c_a(const scalar & c, const vectorfield & a, vectorfield & out)
    {
        int n = out.size();

        // Dot product
        cu_add_c_a2<<<(n+1023)/1024, 1024>>>(c, a.data(), out.data(), n);
        cudaDeviceSynchronize();
    }

    __global__ void cu_add_c_dot(scalar c, Vector3 a, const Vector3 * b, scalar * out, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            out[idx] += c*a.dot(b[idx]);
        }
    }
    // out[i] += c * a*b[i]
    void add_c_dot(const scalar & c, const Vector3 & a, const vectorfield & b, scalarfield & out)
    {
        int n = out.size();

        // Dot product
        cu_add_c_dot<<<(n+1023)/1024, 1024>>>(c, a, b.data(), out.data(), n);
        cudaDeviceSynchronize();
    }

    __global__ void cu_add_c_dot2(scalar c, const Vector3 * a, const Vector3 * b, scalar * out, size_t N)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if(idx < N)
        {
            out[idx] += c*a[idx].dot(b[idx]);
        }
    }
    // out[i] += c * a[i]*b[i]
    void add_c_dot(const scalar & c, const vectorfield & a, const vectorfield & b, scalarfield & out)
    {
        int n = out.size();

        // Dot product
        cu_add_c_dot2<<<(n+1023)/1024, 1024>>>(c, a.data(), b.data(), out.data(), n);
        cudaDeviceSynchronize();
    }


    // out[i] += c * a x b[i]
    void add_c_cross(const scalar & c, const Vector3 & a, const vectorfield & b, vectorfield & out)
    {

    }

    // out[i] += c * a[i] x b[i]
    void add_c_cross(const scalar & c, const vectorfield & a, const vectorfield & b, vectorfield & out)
    {

    }


}

#endif